\selectlanguage{english} 
\begin{abstract}
The recent technological advances in acquisition equipment provided experiments with growing amounts of precise yet affordable sensors; at the same time also a rise of computational power, coming from heterogeneous hardware (GPU, FPGA, TPU), has been made available at relatively low costs. This led us to explore the possibility of completely renewing the chain of acquisition for a fusion experiment, where many high-rate sources of data, coming from different diagnostics, can be combined in a wide framework of computational algorithms performing operations such as feature extraction, compression, denoising, corrupted or missing data recovery and even diagnostic to diagnostic mapping.
Still, if on one side many different diagnostics can be applied, adding new data sources and enriching our knowledge about the physical aspects, on the other hand, the dimensions of the overall model grow making relations among variables more and more difficult. This leads to a statistical challenge called “the curse of dimensionality”. A new approach for the integration of such heterogeneous diagnostics, based on machine learning techniques, could ease this problem. 
The present study aims at proposing the application of the unsupervised approach that recently gained lot of attention thanks to the remarkable results of the so called “generative algorithms” ( such as the Variational Autoencoders ) in latent space representation and disentanglement factors analysis.

The intuitive idea of a non linear control system, made by means of deep generative networks, is inspired on how the brain applies the same sensory integration to perform complex tasks in an "automatic" manner. For example, a complex physical model subtends the simple task of bike riding, however no one apply any model for that. Learning to ride a bike is more related to fusing sensors data and shaping a space of equilibrium by means of small movements around the stability.
The application case has been devoted to the new RFX-mod2 experiment changes where a new set of FPGA devices will be added to the acquisition chain to provide the experiment with new flexible channels, able to adapt to a wider set of machine configurations. These tools of analysis could be eventually considered both to manage the new increased amount of data, and to possibly apply more information to the control system loop. 

However to ensure a real-time feedback control signal for fusion experiments, those algorithmic techniques must be adapted to run in well suited hardware. Thus, the research path followed by this dissertation has been configured as the development of a comprehensive software framework able to handle the deployment of such machine learning algorithms into different hardware architectures. In general terms the binding of a software procedure to a particular hardware is done by a specific set of tools, called compilation tool-chain, matching the language in which the software has been defined with the hardware architecture. The framework focuses on managing all the algorithms and possible tool-chains in a uniform way storing them and gathering results.
In addition it is shown that, attempting a quantization of neurons transfer functions, such models can be modified to create an embedded firmware. Those firmware, approximating the deep generative model to a set of simple operations, fit well with the simple logic units that are largely abundant in FPGAs. This is the key factor that permits the use of affordable hardware with complex deep neural topology and operates them in real-time. 
For this reason, applying these hardware analyzers to the main system loop of the Padova machine could represent a new perspective of control that merges the current physical models with the overall history of the experiment acquisitions. The ultimate goal would be to produce a more solid and effective feedback for RFX-mod2 and other experiments to come.

\end{abstract}


\selectlanguage{italian} 
\begin{abstract}
I recenti progressi tecnologici nel settore delle apparecchiature di acquisizione hanno permesso di fornire agli esperimenti crescenti quantità di sensori precisi a buon mercato; allo stesso tempo anche un aumento della potenza computazionale, proveniente da hardware di varia natura (GPU, FPGA, TPU), è stato reso disponibile a costi relativamente bassi. Questo ci ha portati a considerare la possibilità di rinnovare completamente la catena di acquisizione per un esperimento di fusione, in cui molte fonti di dati ad alta velocità, provenienti da diversi sistemi diagnostici, possono essere combinate in un ampio quadro di algoritmi computazionali che eseguono operazioni come la \textit{feature extraction}, la compressione, il \textit{denoising}, il recupero dei dati danneggiati o mancanti e persino la mappatura \textit{diagnostic} to \textit{diagnostic}.

Tuttavia, se da un lato possono essere applicate molte diagnostiche diversificate, aggiungendo nuove fonti di dati e arricchendo la nostra conoscenza degli aspetti fisici, dall'altro le accresciute dimensioni del modello generale rendono sempre più difficili le relazioni tra le variabili. Ciò porta ad una sfida statistica chiamata “\textit{curse of dimensionality}”. Un nuovo approccio per l'integrazione di una diagnostica così eterogenea, basato su tecniche di apprendimento automatico, potrebbe alleviare questo problema.
Il presente studio mira a proporre l'applicazione di un approccio non supervisionato che recentemente ha attirato molta attenzione grazie ai notevoli risultati dei cosiddetti “algoritmi generativi” (e.g. gli Autoencoder variazionali) nella rappresentazione dello spazio latente e nell'analisi dei \textit{disentanglement factors}.
L'idea intuitiva di un sistema di controllo non lineare, realizzato per mezzo di reti di deep learning, si ispira al modo in cui il cervello applica la stessa integrazione sensoriale per eseguire compiti complessi in modo "automatico". Ad esempio, il semplice compito dell’andare in bicicletta sottende un modello fisico complesso, tuttavia per eseguirlo nessuno si preoccupa di applicarne uno. Imparare ad andare in bicicletta è più legato alla fusione dei dati raccolti dai sensi, e alla determinazione della stabilità mediante piccoli movimenti attorno all’equilibrio.

Il caso applicativo del presente elaborato riguarda le modifiche previste per l'esperimento RFX-mod2 di Padova, in cui un nuovo set di dispositivi FPGA verrà aggiunto alla catena di acquisizione per fornire all'esperimento nuovi canali, in grado di adattarsi a un set più ampio di configurazioni di macchina. Questi strumenti di analisi potrebbero essere usati sia per gestire la nuova maggiore quantità di dati, sia eventualmente per fornire più informazioni al circuito del sistema di controllo.
Negli esperimenti di fusione, tuttavia, per garantire un segnale di controllo in tempo reale, tali tecniche algoritmiche devono essere adattate per funzionare su hardware specifici. Per tale motivo il percorso di ricerca descritto in  questa tesi è stato rivolto allo sviluppo di un \textit{software framework} in grado di gestire l’applicazione di algoritmi di machine learning in diverse architetture hardware. In termini generali, l'associazione di una procedura software a un determinato hardware viene eseguita da un insieme specifico di strumenti, chiamato catena di strumenti di compilazione, che adattano il linguaggio in cui il software è definito all’architettura dell’hardware. Il framework si occupa in modo uniforme della gestione di tutti gli algoritmi e delle possibili catene di strumenti di compilazione, memorizzando e raccogliendo risultati.

Inoltre, operando una quantizzazione delle funzioni di trasferimento dei neuroni, tali modelli possono essere modificati per creare un firmware per dispositivi embedded. Tali firmware, sfruttando l'approssimazione del modello di deep learning ad una serie di semplici operazioni, si adattano bene alle semplici unità logiche che sono ampiamente incorporate nelle FPGA. Questo è il fattore chiave che consente di adottare modelli complessi di machine learning su hardware dai costi ridotti anche per applicazioni in tempo reale. Alla luce di quanto discusso, applicare questi analizzatori hardware al circuito principale del sistema della macchina di Padova potrebbe rappresentare una nuova prospettiva di controllo che unisce gli attuali modelli fisici alla storia complessiva delle acquisizioni dell'esperimento. L’obiettivo finale sarebbe quello di produrre un feedback più solido ed efficace per RFX-mod2 e altri esperimenti a venire.

\end{abstract}
\selectlanguage{english} 